import streamlit as st
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle
import os

# Sayfa ayarlarÄ±
st.set_page_config(
    page_title="TÃ¼rkÃ§e SaÄŸlÄ±k AsistanÄ±",
    page_icon="ğŸ¥",
    layout="wide"
)

# BaÅŸlÄ±k
st.title("ğŸ¥ TÃ¼rkÃ§e SaÄŸlÄ±k Bilgi AsistanÄ±")
st.markdown("**43,000+ tÄ±bbi makaleden bilgi Ã§eken RAG tabanlÄ± chatbot**")
st.markdown("---")

# Session state baÅŸlat (chat history iÃ§in)
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'chat_started' not in st.session_state:
    st.session_state.chat_started = False

# API Key kontrolÃ¼
with st.sidebar:
    st.header("âš™ï¸ Ayarlar")
    
    # Ã–nce environment'tan kontrol et
    api_key = os.environ.get("GEMINI_API_KEY", "")
    
    if api_key:
        st.success("âœ… API Key yÃ¼klendi!")
    else:
        api_key = st.text_input("Gemini API Key", type="password", help="Ãœcretsiz key almak iÃ§in: https://aistudio.google.com/apikey")
        if not api_key:
            st.warning("âš ï¸ LÃ¼tfen API Key giriniz.")
    
    st.markdown("---")
    st.markdown("### ğŸ“Š Proje Bilgileri")
    st.info("""
    - **Dataset:** 42,804 TÃ¼rkÃ§e tÄ±bbi makale
    - **Chunk:** 244,150 parÃ§a
    - **Model:** Gemini 2.5 Flash
    - **Embedding:** Multilingual MiniLM
    - **Vector DB:** FAISS
    """)
    
    if st.button("ğŸ—‘ï¸ Sohbeti Temizle"):
        st.session_state.messages = []
        st.session_state.chat_started = False
        st.rerun()

# API key yoksa dur
if not api_key:
    st.info("ğŸ‘ˆ LÃ¼tfen soldaki menÃ¼den API Key giriniz.")
    st.stop()

# Model ve verileri yÃ¼kle (cache ile)
@st.cache_resource
def load_models_and_data():
    # Embedding model
    embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    
    # FAISS index ve chunks yÃ¼kle
    with open('faiss_index.pkl', 'rb') as f:
        data = pickle.load(f)
    
    index = data['index']
    chunks = data['chunks']
    metadatas = data['metadatas']
    
    return embedding_model, index, chunks, metadatas

try:
    with st.spinner("ğŸ”„ Model yÃ¼kleniyor..."):
        embedding_model, index, chunks, metadatas = load_models_and_data()
    
    # Gemini yapÄ±landÄ±r
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.5-flash')
    
    if not st.session_state.chat_started:
        st.success("âœ… Sistem hazÄ±r! Soru sorabilirsiniz.")
        st.session_state.chat_started = True
    
except Exception as e:
    st.error(f"âŒ Model yÃ¼klenirken hata: {e}")
    st.stop()

# RAG fonksiyonu (CHATBOT tarzÄ±)
def rag_query(question, top_k=3):
    # Soruyu embedding'e Ã§evir
    question_embedding = embedding_model.encode([question])[0]
    question_embedding = np.array([question_embedding]).astype('float32')
    
    # FAISS'te ara
    distances, indices = index.search(question_embedding, top_k)
    
    # Chunk'larÄ± al
    retrieved_chunks = []
    retrieved_titles = []
    
    for idx in indices[0]:
        retrieved_chunks.append(chunks[idx])
        retrieved_titles.append(metadatas[idx]['title'])
    
    # Context oluÅŸtur
    context = "\n\n".join([
        f"[{title}]: {chunk[:300]}..." 
        for title, chunk in zip(retrieved_titles, retrieved_chunks)
    ])
    
    # Chatbot tarzÄ± prompt
    prompt = f"""Sen TÃ¼rkÃ§e konuÅŸan, samimi ve yardÄ±msever bir saÄŸlÄ±k asistanÄ±sÄ±n. 

KAYNAKLARDAN BÄ°LGÄ°LER:
{context}

KULLANICI SORUSU: {question}

Ã–NEMLÄ° KURALLAR:
- KÄ±sa, Ã¶zlÃ¼ ve konuÅŸma tarzÄ±nda cevap ver (maksimum 3-4 cÃ¼mle)
- Gereksiz detaya girme, doÄŸrudan cevapla
- Samimi ol ama profesyonel kal
- Kaynaklarda bilgi yoksa "Bu konuda kaynaklarda yeterli bilgi bulamadÄ±m, lÃ¼tfen bir saÄŸlÄ±k profesyoneline danÄ±ÅŸÄ±n" de
- CevabÄ±n sonunda kaynak baÅŸlÄ±klarÄ±nÄ± belirtme (sistem otomatik gÃ¶sterecek)

CEVAP:"""

    # Gemini'den cevap al
    response = model.generate_content(prompt)
    answer = response.text
    
    unique_sources = list(set(retrieved_titles))[:3]
    
    return answer, unique_sources

# Chat arayÃ¼zÃ¼
st.markdown("### ğŸ’¬ Sohbet")

# Ã–nceki mesajlarÄ± gÃ¶ster
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message:
            with st.expander("ğŸ“š Kaynaklar"):
                for i, source in enumerate(message["sources"], 1):
                    st.markdown(f"{i}. {source}")

# Yeni mesaj giriÅŸi
if prompt := st.chat_input("SaÄŸlÄ±k hakkÄ±nda bir soru sorun..."):
    # KullanÄ±cÄ± mesajÄ±nÄ± ekle
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Bot cevabÄ±
    with st.chat_message("assistant"):
        with st.spinner("ğŸ¤” DÃ¼ÅŸÃ¼nÃ¼yorum..."):
            try:
                answer, sources = rag_query(prompt, top_k=3)
                st.markdown(answer)
                
                with st.expander("ğŸ“š Kaynaklar"):
                    for i, source in enumerate(sources, 1):
                        st.markdown(f"{i}. {source}")
                
                # MesajÄ± kaydet
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": answer,
                    "sources": sources
                })
                
            except Exception as e:
                st.error(f"âŒ Hata oluÅŸtu: {e}")

# Ã–rnek sorular (ilk baÅŸta gÃ¶ster)
if len(st.session_state.messages) == 0:
    st.markdown("---")
    st.markdown("### ğŸ’¡ Ã–rnek Sorular:")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.button("ğŸ©º Diyabet nedir?", key="ex1", use_container_width=True)
    with col2:
        st.button("ğŸ’Š Migren nasÄ±l geÃ§er?", key="ex2", use_container_width=True)
    with col3:
        st.button("ğŸ¤° Hamilelik testleri", key="ex3", use_container_width=True)
