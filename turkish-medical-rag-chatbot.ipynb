{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🏥 Türkçe Sağlık Bilgi Asistanı - RAG Chatbot\n\n## 📊 Proje Hakkında\nBu projede, **43,000 Türkçe tıbbi makale** kullanarak bir **RAG (Retrieval Augmented Generation)** tabanlı chatbot geliştireceğiz.\n\n**Dataset:** alibayram/turkish-medical-articles (doktorsitesi.com)\n\n**Teknolojiler:**\n- 🤖 **LLM:** Google Gemini API\n- 🔍 **Embedding:** Sentence Transformers\n- 📦 **Vector Database:** FAISS\n- 🔗 **Framework:** LangChain\n\n**Proje Akışı:**\n1. Dataset yükleme ve keşif\n2. Veri temizleme\n3. Text chunking (metinleri parçalara böl)\n4. Embedding ve vector store oluşturma\n5. RAG pipeline kurulumu\n6. Test ve demo","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Kütüphane Kurulumu\n# Bu cell'i çalıştırdıktan sonra \"Restart kernel\" yapman gerekebilir\n\n!pip install -q datasets huggingface_hub\n!pip install -q langchain langchain-community langchain-google-genai\n!pip install -q sentence-transformers\n!pip install -q faiss-cpu\n!pip install -q google-generativeai\n!pip install -q chromadb\n\nprint(\"✅ Tüm kütüphaneler başarıyla yüklendi!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:50:19.424592Z","iopub.execute_input":"2025-10-20T14:50:19.425293Z","iopub.status.idle":"2025-10-20T14:50:43.716928Z","shell.execute_reply.started":"2025-10-20T14:50:19.425267Z","shell.execute_reply":"2025-10-20T14:50:43.715969Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.38.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ngoogle-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ngoogle-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.8.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-google-genai 2.1.12 requires google-ai-generativelanguage<1,>=0.7, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\nlangchain-google-genai 2.1.12 requires google-ai-generativelanguage<1,>=0.7, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\ngoogle-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✅ Tüm kütüphaneler başarıyla yüklendi!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 📂 Adım 1: Dataset Yükleme\n\nHugging Face'ten **turkish-medical-articles** dataset'ini yüklüyoruz.\nBu dataset 43,000 Türkçe tıbbi makale içeriyor.","metadata":{}},{"cell_type":"code","source":"# Hugging Face'e giriş yap (Kaggle Secrets ile)\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# Secret'tan token al\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\n# Giriş yap\nlogin(token=HF_TOKEN)\n\nprint(\"✅ Hugging Face'e giriş yapıldı!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:50:43.718811Z","iopub.execute_input":"2025-10-20T14:50:43.719134Z","iopub.status.idle":"2025-10-20T14:50:43.863122Z","shell.execute_reply.started":"2025-10-20T14:50:43.719111Z","shell.execute_reply":"2025-10-20T14:50:43.862392Z"}},"outputs":[{"name":"stdout","text":"✅ Hugging Face'e giriş yapıldı!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Dataset'i Hugging Face'ten yükle\nfrom datasets import load_dataset\nimport pandas as pd\n\nprint(\"📥 Dataset yükleniyor...\")\n\n# Dataset'i yükle (TIMEOUT ARTIRILMIŞ)\nimport datasets\ndatasets.config.HF_DATASETS_OFFLINE = False\n\n# Timeout'u artır\nfrom huggingface_hub import hf_api\nhf_api.HfApi().timeout = 60  # 60 saniye\n\ndataset = load_dataset(\"umutertugrul/turkish-medical-articles\", download_mode=\"force_redownload\")\n\n# Train split'ini al\ndf = pd.DataFrame(dataset['train'])\n\nprint(f\"✅ Dataset yüklendi!\")\nprint(f\"📊 Toplam makale sayısı: {len(df)}\")\nprint(f\"📋 Sütunlar: {df.columns.tolist()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:50:43.863991Z","iopub.execute_input":"2025-10-20T14:50:43.864280Z","iopub.status.idle":"2025-10-20T14:50:49.268355Z","shell.execute_reply.started":"2025-10-20T14:50:43.864257Z","shell.execute_reply":"2025-10-20T14:50:49.267677Z"}},"outputs":[{"name":"stdout","text":"📥 Dataset yükleniyor...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"doktorsitesi_articles.parquet:   0%|          | 0.00/107M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71d2f3a22f0046c3bbc875b059c5e095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sample.parquet:   0%|          | 0.00/2.62M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"477c663a150d411cac41d6fbb75acbcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/42804 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3e4b210d12247c0af81ac56754f7b95"}},"metadata":{}},{"name":"stdout","text":"✅ Dataset yüklendi!\n📊 Toplam makale sayısı: 42804\n📋 Sütunlar: ['url', 'title', 'text', 'name', 'branch', 'publish_date', 'scrape_date']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 🔍 Adım 2: Veriyi İnceleyelim\n\nDataset'in yapısını ve içeriğini anlayalım.","metadata":{}},{"cell_type":"code","source":"# Veri keşfi (Exploratory Data Analysis)\n\n# İlk 5 satıra bakalım\nprint(\"📄 İlk 5 makale:\")\nprint(df.head())\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# Sütun bilgileri\nprint(\"📊 Sütun bilgileri:\")\nprint(df.info())\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# Null (boş) değerleri kontrol et\nprint(\"❓ Eksik değerler:\")\nprint(df.isnull().sum())\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# Örnek bir makaleyi tamamen görelim\nprint(\"📰 Örnek Makale:\")\nprint(f\"Başlık: {df.iloc[0]['title']}\")\nprint(f\"Yazar: {df.iloc[0]['name']}\")\nprint(f\"Dal: {df.iloc[0]['branch']}\")\nprint(f\"Tarih: {df.iloc[0]['publish_date']}\")\nprint(f\"URL: {df.iloc[0]['url']}\")\nprint(f\"\\nİçerik (ilk 500 karakter):\\n{df.iloc[0]['text'][:500]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:50:49.269360Z","iopub.execute_input":"2025-10-20T14:50:49.269651Z","iopub.status.idle":"2025-10-20T14:50:49.365326Z","shell.execute_reply.started":"2025-10-20T14:50:49.269620Z","shell.execute_reply":"2025-10-20T14:50:49.364672Z"}},"outputs":[{"name":"stdout","text":"📄 İlk 5 makale:\n                                                 url  \\\n0  https://www.doktorsitesi.com/blog/makale/miyom...   \n1  https://www.doktorsitesi.com/blog/makale/parma...   \n2  https://www.doktorsitesi.com/blog/makale/katar...   \n3  https://www.doktorsitesi.com/blog/makale/amalg...   \n4  https://www.doktorsitesi.com/blog/makale/urtik...   \n\n                                      title  \\\n0             Miyom Belirtileri ve Tedavisi   \n1  Parmaklarla konuşmak :online psikoterapi   \n2                      Katarakt ne demektir   \n3                  Amalgam Dolgu Zararlı mı   \n4                     Ürtiker ve anjiyoödem   \n\n                                                text                     name  \\\n0  ➡️Miyomlar rahim kası kaynaklı iyi huylu tümör...  Op. Dr. Ülker Heydarova   \n1  Online PsikoterapiBu yüzyılın başındayken raha...   Dr. Psk. Murat Sarısoy   \n2  Katarakt, göz içindeki lensin saydamlığını kay...   Prof. Dr. Şengül Özdek   \n3  Özellikle son zamanlarda daha sık tartışılan b...   Dt. Tuba Uluneke Uygun   \n4  Deride değişik büyüklüklerde olabilen, hafifçe...    Prof. Dr. Osman Şener   \n\n                        branch    publish_date scrape_date  \n0  Kadın Hastalıkları ve Doğum    8 Eylül 2021  2025-07-09  \n1                    Psikoloji   15 Eylül 2009  2025-07-09  \n2             Göz Hastalıkları  19 Aralık 2017  2025-07-09  \n3                   Diş Hekimi    3 Nisan 2013  2025-07-09  \n4   Dahiliye - İç Hastalıkları    7 Şubat 2016  2025-07-09  \n\n================================================================================\n\n📊 Sütun bilgileri:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 42804 entries, 0 to 42803\nData columns (total 7 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   url           42804 non-null  object\n 1   title         42804 non-null  object\n 2   text          41865 non-null  object\n 3   name          42793 non-null  object\n 4   branch        42775 non-null  object\n 5   publish_date  42803 non-null  object\n 6   scrape_date   42804 non-null  object\ndtypes: object(7)\nmemory usage: 2.3+ MB\nNone\n\n================================================================================\n\n❓ Eksik değerler:\nurl               0\ntitle             0\ntext            939\nname             11\nbranch           29\npublish_date      1\nscrape_date       0\ndtype: int64\n\n================================================================================\n\n📰 Örnek Makale:\nBaşlık: Miyom Belirtileri ve Tedavisi\nYazar: Op. Dr. Ülker Heydarova\nDal: Kadın Hastalıkları ve Doğum\nTarih: 8 Eylül 2021\nURL: https://www.doktorsitesi.com/blog/makale/miyom-belirtileri-ve-tedavisi\n\nİçerik (ilk 500 karakter):\n➡️Miyomlar rahim kası kaynaklı iyi huylu tümörlerdir➡️Miyomlar sıklıkla bulgu vermezler, rutin jünekolojik müayenede veya başka bir nedenle yapılan radyolojik incelemelerde ortaya çıkar➡️En sık görülen bulgular düzensiz ve yoğun adet kanamalarıdır➡️Büyük boyutlara ulaşmış myomlar idrar kesesine ve bağırsaklara bası yaparak sık idrara çıkma, ağrılı dışkılama ve ağrılı ilişki gibi sorunlara neden olabilmektedir➡️Miyomların tedavisi ilaç ve cerrahi yöntemlerle yapılmaktadır. Tedavi seçiminde hastan...\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 🧹 Adım 3: Veri Temizleme\n\nBoş (null) text değerlerini çıkaralım ve sadece gerekli sütunları tutalım.\nRAG için sadece **title** ve **text** sütunlarına ihtiyacımız var.","metadata":{}},{"cell_type":"code","source":"# Veri temizleme\n\n# 1. Text'i boş olan satırları çıkar\ndf_clean = df[df['text'].notna()].copy()\n\nprint(f\"✅ Temizleme öncesi: {len(df)} makale\")\nprint(f\"✅ Temizleme sonrası: {len(df_clean)} makale\")\nprint(f\"🗑️ Silinen: {len(df) - len(df_clean)} makale\")\n\n# 2. Sadece gerekli sütunları tut (title ve text)\ndf_clean = df_clean[['title', 'text']].copy()\n\n# 3. Text uzunluklarına bakalım\ndf_clean['text_length'] = df_clean['text'].str.len()\n\nprint(\"\\n📏 Metin uzunluk istatistikleri:\")\nprint(df_clean['text_length'].describe())\n\n# 4. Çok kısa metinleri filtrele (100 karakterden kısa olanlar muhtemelen hatalı)\ndf_clean = df_clean[df_clean['text_length'] > 100].copy()\n\nprint(f\"\\n✅ Çok kısa metinler temizlendi\")\nprint(f\"📊 Final veri sayısı: {len(df_clean)} makale\")\n\n# text_length sütununu sil (artık gerek yok)\ndf_clean = df_clean[['title', 'text']].copy()\n\nprint(\"\\n✅ Veri temizleme tamamlandı!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:50:49.367221Z","iopub.execute_input":"2025-10-20T14:50:49.367832Z","iopub.status.idle":"2025-10-20T14:50:49.663304Z","shell.execute_reply.started":"2025-10-20T14:50:49.367812Z","shell.execute_reply":"2025-10-20T14:50:49.662654Z"}},"outputs":[{"name":"stdout","text":"✅ Temizleme öncesi: 42804 makale\n✅ Temizleme sonrası: 41865 makale\n🗑️ Silinen: 939 makale\n\n📏 Metin uzunluk istatistikleri:\ncount     41865.000000\nmean       4120.693849\nstd        5461.719662\nmin           2.000000\n25%        1826.000000\n50%        3002.000000\n75%        4827.000000\nmax      336572.000000\nName: text_length, dtype: float64\n\n✅ Çok kısa metinler temizlendi\n📊 Final veri sayısı: 41788 makale\n\n✅ Veri temizleme tamamlandı!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## ✂️ Adım 4: Text Chunking (Metin Bölme)\n\nRAG sisteminde uzun metinleri küçük parçalara (chunks) böleriz.\nBu sayede:\n- 🎯 Daha hassas arama yapılır\n- 💾 Embedding modeli daha iyi çalışır\n- 🔍 İlgili bilgi daha kolay bulunur\n\n**Parametreler:**\n- **Chunk size:** 1000 karakter (her parça max 1000 karakter)\n- **Overlap:** 200 karakter (parçalar arasında 200 karakter örtüşme)","metadata":{}},{"cell_type":"code","source":"# Text Chunking - Metinleri parçalara böl\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nprint(\"✂️ Metinler parçalara bölünüyor...\")\n\n# Text splitter oluştur\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,        # Her parça max 1000 karakter\n    chunk_overlap=200,      # Parçalar arası 200 karakter örtüşme\n    length_function=len,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Önce paragraflardan böl, sonra cümlelerden\n)\n\n# Tüm metinleri birleştir ve metadata'ya title ekle\nchunks = []\nmetadatas = []\n\nfor idx, row in df_clean.iterrows():\n    # Her makaleyi parçala\n    text_chunks = text_splitter.split_text(row['text'])\n    \n    # Her parça için metadata ekle\n    for chunk in text_chunks:\n        chunks.append(chunk)\n        metadatas.append({\n            'title': row['title'],\n            'chunk_id': len(chunks) - 1\n        })\n\nprint(f\"✅ Chunking tamamlandı!\")\nprint(f\"📄 Toplam makale: {len(df_clean)}\")\nprint(f\"✂️ Toplam chunk: {len(chunks)}\")\nprint(f\"📊 Makale başına ortalama chunk: {len(chunks) / len(df_clean):.1f}\")\n\n# Örnek bir chunk göster\nprint(f\"\\n📝 Örnek Chunk:\")\nprint(f\"Başlık: {metadatas[0]['title']}\")\nprint(f\"İçerik (ilk 300 karakter):\\n{chunks[0][:300]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:50:49.664009Z","iopub.execute_input":"2025-10-20T14:50:49.664201Z","iopub.status.idle":"2025-10-20T14:51:00.886192Z","shell.execute_reply.started":"2025-10-20T14:50:49.664181Z","shell.execute_reply":"2025-10-20T14:51:00.885460Z"}},"outputs":[{"name":"stdout","text":"✂️ Metinler parçalara bölünüyor...\n✅ Chunking tamamlandı!\n📄 Toplam makale: 41788\n✂️ Toplam chunk: 244150\n📊 Makale başına ortalama chunk: 5.8\n\n📝 Örnek Chunk:\nBaşlık: Miyom Belirtileri ve Tedavisi\nİçerik (ilk 300 karakter):\n➡️Miyomlar rahim kası kaynaklı iyi huylu tümörlerdir➡️Miyomlar sıklıkla bulgu vermezler, rutin jünekolojik müayenede veya başka bir nedenle yapılan radyolojik incelemelerde ortaya çıkar➡️En sık görülen bulgular düzensiz ve yoğun adet kanamalarıdır➡️Büyük boyutlara ulaşmış myomlar idrar kesesine ve b...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 🧠 Adım 5: Embedding Model\n\nMetinleri **vektörlere** (sayısal dizilere) çevireceğiz.\nBu sayede benzerlik araması yapabileceğiz.\n\n**Model:** `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`\n- ✅ Türkçe destekler\n- ✅ Küçük ve hızlı\n- ✅ Ücretsiz","metadata":{}},{"cell_type":"code","source":"# Embedding model yükle\n\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nprint(\"🧠 Embedding model yükleniyor...\")\n\n# Türkçe destekleyen multilingual model\nembedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n\nprint(\"✅ Model yüklendi!\")\n\n# Test edelim - örnek bir cümleyi embedding'e çevir\ntest_text = \"Diyabet nedir?\"\ntest_embedding = embedding_model.encode(test_text)\n\nprint(f\"\\n🧪 Test:\")\nprint(f\"Metin: '{test_text}'\")\nprint(f\"Embedding boyutu: {len(test_embedding)}\")\nprint(f\"Embedding örneği (ilk 10 değer): {test_embedding[:10]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:51:00.886953Z","iopub.execute_input":"2025-10-20T14:51:00.887180Z","iopub.status.idle":"2025-10-20T14:51:35.372368Z","shell.execute_reply.started":"2025-10-20T14:51:00.887164Z","shell.execute_reply":"2025-10-20T14:51:35.371741Z"}},"outputs":[{"name":"stderr","text":"2025-10-20 14:51:13.208017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760971873.419185      75 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760971873.484365      75 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"🧠 Embedding model yükleniyor...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9936149af32c4dde9968863fbd8c54aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0b891bd50e4d82b24831cc3c24905d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c3e20aa7f064be38f2a78c3e5e05f51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8842b27e2f3d4f48affe5f2ad7be21a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b5dd1ed148463e874bd74b84aa4816"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6a51d73a2a440e88750b3b69899d604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7b8e6fa2b9a450ea496158903cb4410"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1112e10c0f24f5cbe450adae576c448"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b815d135c6b54dcfbbef580460e688e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93f030eca17545938386d72db3c128da"}},"metadata":{}},{"name":"stdout","text":"✅ Model yüklendi!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95a032ca78354df18f1cbad0340574ee"}},"metadata":{}},{"name":"stdout","text":"\n🧪 Test:\nMetin: 'Diyabet nedir?'\nEmbedding boyutu: 384\nEmbedding örneği (ilk 10 değer): [ 0.13761216  0.01697462 -0.08340565 -0.00484816 -0.01542479 -0.22697556\n  0.43093085 -0.07262544  0.03188347  0.08843873]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 📦 Adım 6: Vector Database (FAISS)\n\nTüm chunk'ları embedding'e çevirip **FAISS** veritabanına kaydediyoruz.\nFAISS sayesinde milyonlarca vektör arasında **hızlı arama** yapabiliriz.\n\n⚠️ **Not:** 244,150 chunk için embedding oluşturma ~10-15 dakika sürebilir.","metadata":{}},{"cell_type":"code","source":"# FAISS Vector Database Oluştur\n\nimport faiss\nfrom tqdm import tqdm\n\nprint(\"📦 Tüm chunk'lar embedding'e çevriliyor...\")\nprint(\"⏳ Bu işlem 10-15 dakika sürebilir, lütfen bekleyin...\\n\")\n\n# Tüm chunk'ları embedding'e çevir (batch processing için)\nbatch_size = 100  # Aynı anda 100 chunk işle\nall_embeddings = []\n\nfor i in tqdm(range(0, len(chunks), batch_size)):\n    batch = chunks[i:i+batch_size]\n    batch_embeddings = embedding_model.encode(batch, show_progress_bar=False)\n    all_embeddings.extend(batch_embeddings)\n\n# Numpy array'e çevir\nembeddings_array = np.array(all_embeddings).astype('float32')\n\nprint(f\"\\n✅ Embedding'ler oluşturuldu!\")\nprint(f\"📊 Shape: {embeddings_array.shape}\")\n\n# FAISS index oluştur\nprint(\"\\n📦 FAISS index oluşturuluyor...\")\ndimension = embeddings_array.shape[1]  # 384\nindex = faiss.IndexFlatL2(dimension)  # L2 (Euclidean) mesafe\nindex.add(embeddings_array)\n\nprint(f\"✅ FAISS index oluşturuldu!\")\nprint(f\"📊 Toplam vektör sayısı: {index.ntotal}\")\n\nprint(\"\\n🎉 Vector Database hazır!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:51:35.373231Z","iopub.execute_input":"2025-10-20T14:51:35.373562Z","iopub.status.idle":"2025-10-20T15:00:56.762379Z","shell.execute_reply.started":"2025-10-20T14:51:35.373537Z","shell.execute_reply":"2025-10-20T15:00:56.761670Z"}},"outputs":[{"name":"stdout","text":"📦 Tüm chunk'lar embedding'e çevriliyor...\n⏳ Bu işlem 10-15 dakika sürebilir, lütfen bekleyin...\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2442/2442 [09:20<00:00,  4.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n✅ Embedding'ler oluşturuldu!\n📊 Shape: (244150, 384)\n\n📦 FAISS index oluşturuluyor...\n✅ FAISS index oluşturuldu!\n📊 Toplam vektör sayısı: 244150\n\n🎉 Vector Database hazır!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 🤖 Adım 7: RAG Pipeline - Gemini API Entegrasyonu\n\nŞimdi tüm parçaları birleştiriyoruz:\n1. 🔍 Kullanıcı soru sorar\n2. 🧠 Soruyu embedding'e çeviririz\n3. 📦 FAISS'te en benzer chunk'ları buluruz\n4. 🤖 Gemini'ye chunk'ları + soruyu gönderip cevap alırız","metadata":{}},{"cell_type":"code","source":"# Gemini API Kurulumu (Kaggle Secrets ile)\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\n# Secret'tan API key al\nuser_secrets = UserSecretsClient()\nGEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\ngenai.configure(api_key=GEMINI_API_KEY)\n\n# Gemini model oluştur\nmodel = genai.GenerativeModel('gemini-2.5-flash')\n\nprint(\"✅ Gemini API hazır!\")\n\n# Test edelim\ntest_response = model.generate_content(\"Merhaba, nasılsın?\")\nprint(f\"\\n🧪 Test:\")\nprint(f\"Cevap: {test_response.text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:00:56.763211Z","iopub.execute_input":"2025-10-20T15:00:56.763479Z","iopub.status.idle":"2025-10-20T15:00:58.217467Z","shell.execute_reply.started":"2025-10-20T15:00:56.763459Z","shell.execute_reply":"2025-10-20T15:00:58.216581Z"}},"outputs":[{"name":"stdout","text":"✅ Gemini API hazır!\n\n🧪 Test:\nCevap: Merhaba! Ben iyiyim, teşekkür ederim. Sen nasılsın?\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 🔗 Adım 8: RAG Fonksiyonu - Soru Cevaplama\n\nŞimdi tüm parçaları birleştirip soru-cevap fonksiyonunu oluşturuyoruz!","metadata":{}},{"cell_type":"code","source":"# RAG Soru-Cevap Fonksiyonu\n\ndef rag_query(question, top_k=5):\n    \"\"\"\n    RAG sistemi ile soru cevaplama\n    \n    Args:\n        question: Kullanıcının sorusu\n        top_k: Kaç chunk getirileceği (varsayılan 5)\n    \n    Returns:\n        answer: Gemini'nin cevabı\n        sources: Kullanılan kaynaklar\n    \"\"\"\n    \n    # 1. Soruyu embedding'e çevir\n    question_embedding = embedding_model.encode([question])[0]\n    question_embedding = np.array([question_embedding]).astype('float32')\n    \n    # 2. FAISS'te en benzer chunk'ları bul\n    distances, indices = index.search(question_embedding, top_k)\n    \n    # 3. İlgili chunk'ları ve başlıklarını al\n    retrieved_chunks = []\n    retrieved_titles = []\n    \n    for idx in indices[0]:\n        retrieved_chunks.append(chunks[idx])\n        retrieved_titles.append(metadatas[idx]['title'])\n    \n    # 4. Context oluştur (chunk'ları birleştir)\n    context = \"\\n\\n---\\n\\n\".join([\n        f\"Kaynak: {title}\\n{chunk}\" \n        for title, chunk in zip(retrieved_titles, retrieved_chunks)\n    ])\n    \n    # 5. Gemini'ye prompt gönder\n    prompt = f\"\"\"Sen Türkçe tıbbi bir asistansın. Aşağıdaki tıbbi makalelerden yararlanarak soruyu cevapla.\n\nKAYNAKLARDAN ALINAN BİLGİLER:\n{context}\n\nSORU: {question}\n\nCEVAP: Yukarıdaki kaynaklara dayanarak, soruyu açık ve anlaşılır bir şekilde Türkçe olarak cevaplayın. Eğer kaynaklarda bilgi yoksa, \"Bu konuda kaynaklarda yeterli bilgi bulunamadı\" deyin.\"\"\"\n\n    # 6. Gemini'den cevap al\n    response = model.generate_content(prompt)\n    answer = response.text\n    \n    # 7. Benzersiz başlıkları bul (kaynak olarak göstermek için)\n    unique_sources = list(set(retrieved_titles))\n    \n    return answer, unique_sources\n\nprint(\"✅ RAG fonksiyonu hazır!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:00:58.218303Z","iopub.execute_input":"2025-10-20T15:00:58.218580Z","iopub.status.idle":"2025-10-20T15:00:58.225169Z","shell.execute_reply.started":"2025-10-20T15:00:58.218561Z","shell.execute_reply":"2025-10-20T15:00:58.224371Z"}},"outputs":[{"name":"stdout","text":"✅ RAG fonksiyonu hazır!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## 🧪 Adım 9: RAG Sistemini Test Edelim!\n\nSisteme örnek sorular soralım ve cevapları görelim.","metadata":{}},{"cell_type":"code","source":"# RAG Sistemini Test Et\n\nprint(\"🧪 RAG Sistemi Test Ediliyor...\\n\")\nprint(\"=\"*80)\n\n# Test Sorusu 1\nquestion1 = \"Diyabet nedir ve belirtileri nelerdir?\"\nprint(f\"\\n❓ SORU 1: {question1}\\n\")\n\nanswer1, sources1 = rag_query(question1, top_k=3)\nprint(f\"🤖 CEVAP:\\n{answer1}\\n\")\nprint(f\"📚 KAYNAKLAR: {', '.join(sources1[:3])}\")\n\nprint(\"\\n\" + \"=\"*80)\n\n# Test Sorusu 2\nquestion2 = \"Migren ağrısı nasıl geçer?\"\nprint(f\"\\n❓ SORU 2: {question2}\\n\")\n\nanswer2, sources2 = rag_query(question2, top_k=3)\nprint(f\"🤖 CEVAP:\\n{answer2}\\n\")\nprint(f\"📚 KAYNAKLAR: {', '.join(sources2[:3])}\")\n\nprint(\"\\n\" + \"=\"*80)\n\n# Test Sorusu 3\nquestion3 = \"Hamilelerde yapılan testler nelerdir?\"\nprint(f\"\\n❓ SORU 3: {question3}\\n\")\n\nanswer3, sources3 = rag_query(question3, top_k=3)\nprint(f\"🤖 CEVAP:\\n{answer3}\\n\")\nprint(f\"📚 KAYNAKLAR: {', '.join(sources3[:3])}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\n✅ Test tamamlandı! RAG sistemi başarıyla çalışıyor! 🎉\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:00:58.226275Z","iopub.execute_input":"2025-10-20T15:00:58.226582Z","iopub.status.idle":"2025-10-20T15:01:13.588351Z","shell.execute_reply.started":"2025-10-20T15:00:58.226559Z","shell.execute_reply":"2025-10-20T15:01:13.587441Z"}},"outputs":[{"name":"stdout","text":"🧪 RAG Sistemi Test Ediliyor...\n\n================================================================================\n\n❓ SORU 1: Diyabet nedir ve belirtileri nelerdir?\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8823f15fe87640ccb4d2308c9b0bac31"}},"metadata":{}},{"name":"stdout","text":"🤖 CEVAP:\nBu konuda kaynaklarda yeterli bilgi bulunamadı. Kaynaklar, ülseratif kolit ve Crohn hastalığı hakkında bilgi içermektedir.\n\n📚 KAYNAKLAR: Ülseratif  kolit  ve crohn  hastalığı hakkında, Ülseratif kolit hakkında merak edilenler\n\n================================================================================\n\n❓ SORU 2: Migren ağrısı nasıl geçer?\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be893e3b9e56421498d17af2180f71be"}},"metadata":{}},{"name":"stdout","text":"🤖 CEVAP:\nYukarıdaki kaynaklara göre, migren ağrısının nasıl geçtiğine dair doğrudan ve detaylı bir açıklama bulunmamaktadır. Ancak bir kaynakta şu bilgi yer almaktadır:\n\n*   \"Uyuyabilirse çoğu hastada ağrı hafifler ve geçer.\"\n\nBu bilgiye göre, birçok migren hastasında ağrı, uyuyabildiğinde hafiflemekte ve geçmektedir. Hastalar genellikle loş, sessiz bir odada istirahati tercih ederler, bu da ağrının hafiflemesine yardımcı olabilecek bir durumdur.\n\n📚 KAYNAKLAR: Hava değişimleri baş ağrısını tetikliyor, Migren !, Migren nedir, kesin çözümü var mıdır?\n\n================================================================================\n\n❓ SORU 3: Hamilelerde yapılan testler nelerdir?\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"683b9946e7a64c0e8e17d3bd176e5791"}},"metadata":{}},{"name":"stdout","text":"🤖 CEVAP:\nHamilelerde yapılan testler, kaynaklarda belirtildiği üzere başlıca şu şekildedir:\n\n1.  **Gebeliğin Tespiti ve Normal Seyrinin Belirlenmesi:**\n    *   Adet gecikmesi durumunda idrar veya kanla yapılan tahliller yardımıyla gebeliğin varlığı tespit edilir.\n    *   Gebelik testleri pozitifse, dış gebelik, boş gebelik veya mol (üzüm) gebeliği gibi durumları ekarte etmek ve gebeliğin normal olup olmadığını belirlemek için testler yapılır.\n\n2.  **Gebelik Tarama Testleri (Fetüsteki Kromozomal ve Genetik Hastalık Riskini Değerlendirmek İçin):**\n    *   **Birinci Trimester Tarama Testi:** Anneye kan testi ve fetüse ense kalınlığı ölçümü yapılarak Down Sendromu ve diğer kromozomal anomaliler açısından risk skoru oluşturulur.\n    *   **İkinci Trimester Tarama Testi (Üçlü/Dörtlü Test):** Anneye kan testi ve fetüse ense kalınlığı ölçümü yapılarak Down Sendromu, Açık Nöral Tüp Defekti ve Trisomi 18 açısından risk değerlendirmesi yapılır.\n    *   **Harmonik Tarama Testi:** Anneye yapılan tek kan testi ile plasentadan salgılanan özel proteinlerin seviyeleri ölçülerek Down Sendromu, Trisomi 18 ve Trisomi 13 açısından risk değerlendirmesi yapılır.\n\n3.  **Rutin Prenatal Kontroller Sırasında Yapılanlar:**\n    *   Tansiyon ölçümü\n    *   Kilo takibi\n    *   Ultrasonografi\n    *   Hemoglobin analizleri\n    *   İdrar analizleri\n\nBu testler düzenli prenatal kontroller esnasında yapılır ve sonuçlar önceki bulgularla kıyaslanır.\n\n📚 KAYNAKLAR: Normal Gebelik Takibi!!, Gebelik Tarama Testleri, Gebelik takibi hangi sıklıkla olmalı?\n\n================================================================================\n\n✅ Test tamamlandı! RAG sistemi başarıyla çalışıyor! 🎉\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 🌐 Adım 10: Streamlit Web Arayüzü\n\nŞimdi kullanıcıların kolayca kullanabileceği bir web arayüzü oluşturacağız.\nBu arayüz Streamlit ile hazırlanacak ve deploy edilecek.","metadata":{}},{"cell_type":"code","source":"# Streamlit Web Arayüzü Kodu Oluştur\n\nstreamlit_code = '''\nimport streamlit as st\nimport google.generativeai as genai\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\nimport pickle\n\n# Sayfa ayarları\nst.set_page_config(\n    page_title=\"Türkçe Sağlık Asistanı\",\n    page_icon=\"🏥\",\n    layout=\"wide\"\n)\n\n# Başlık\nst.title(\"🏥 Türkçe Sağlık Bilgi Asistanı\")\nst.markdown(\"**43,000+ tıbbi makaleden bilgi çeken RAG tabanlı chatbot**\")\nst.markdown(\"---\")\n\n# API Key girişi (sidebar)\nwith st.sidebar:\n    st.header(\"⚙️ Ayarlar\")\n    api_key = st.text_input(\"Gemini API Key\", type=\"password\")\n    \n    st.markdown(\"---\")\n    st.markdown(\"### 📊 Proje Bilgileri\")\n    st.info(\"\"\"\n    - **Dataset:** 42,804 Türkçe tıbbi makale\n    - **Chunk:** 244,150 parça\n    - **Model:** Gemini 2.5 Flash\n    - **Embedding:** Multilingual MiniLM\n    - **Vector DB:** FAISS\n    \"\"\")\n\n# Ana içerik\nif not api_key:\n    st.warning(\"⚠️ Lütfen soldaki menüden Gemini API Key giriniz.\")\n    st.stop()\n\n# Model ve verileri yükle (cache ile)\n@st.cache_resource\ndef load_models_and_data():\n    # Embedding model\n    embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    \n    # FAISS index ve chunks yükle\n    with open('faiss_index.pkl', 'rb') as f:\n        data = pickle.load(f)\n    \n    index = data['index']\n    chunks = data['chunks']\n    metadatas = data['metadatas']\n    \n    return embedding_model, index, chunks, metadatas\n\ntry:\n    embedding_model, index, chunks, metadatas = load_models_and_data()\n    \n    # Gemini yapılandır\n    genai.configure(api_key=api_key)\n    model = genai.GenerativeModel('gemini-2.5-flash')\n    \n    st.success(\"✅ Sistem hazır! Soru sorabilirsiniz.\")\n    \nexcept Exception as e:\n    st.error(f\"❌ Model yüklenirken hata: {e}\")\n    st.stop()\n\n# Soru-cevap fonksiyonu\ndef rag_query(question, top_k=5):\n    # Soruyu embedding'e çevir\n    question_embedding = embedding_model.encode([question])[0]\n    question_embedding = np.array([question_embedding]).astype('float32')\n    \n    # FAISS'te ara\n    distances, indices = index.search(question_embedding, top_k)\n    \n    # Chunk'ları al\n    retrieved_chunks = []\n    retrieved_titles = []\n    \n    for idx in indices[0]:\n        retrieved_chunks.append(chunks[idx])\n        retrieved_titles.append(metadatas[idx]['title'])\n    \n    # Context oluştur\n    context = \"\\\\n\\\\n---\\\\n\\\\n\".join([\n        f\"Kaynak: {title}\\\\n{chunk}\" \n        for title, chunk in zip(retrieved_titles, retrieved_chunks)\n    ])\n    \n    # Prompt\n    prompt = f\"\"\"Sen Türkçe tıbbi bir asistansın. Aşağıdaki tıbbi makalelerden yararlanarak soruyu cevapla.\n\nKAYNAKLARDAN ALINAN BİLGİLER:\n{context}\n\nSORU: {question}\n\nCEVAP: Yukarıdaki kaynaklara dayanarak, soruyu açık ve anlaşılır bir şekilde Türkçe olarak cevaplayın.\"\"\"\n\n    # Gemini'den cevap al\n    response = model.generate_content(prompt)\n    answer = response.text\n    \n    unique_sources = list(set(retrieved_titles))\n    \n    return answer, unique_sources\n\n# Soru girişi\nst.markdown(\"### 💬 Sorunuzu Sorun\")\n\ncol1, col2 = st.columns([4, 1])\n\nwith col1:\n    question = st.text_input(\"\", placeholder=\"Örn: Diyabet nedir ve belirtileri nelerdir?\")\n\nwith col2:\n    top_k = st.selectbox(\"Kaynak sayısı\", [3, 5, 7], index=1)\n\nif st.button(\"🔍 Sorgula\", type=\"primary\"):\n    if question:\n        with st.spinner(\"🤔 Cevap hazırlanıyor...\"):\n            try:\n                answer, sources = rag_query(question, top_k=top_k)\n                \n                st.markdown(\"### 🤖 Cevap\")\n                st.markdown(answer)\n                \n                st.markdown(\"---\")\n                st.markdown(\"### 📚 Kullanılan Kaynaklar\")\n                for i, source in enumerate(sources[:5], 1):\n                    st.markdown(f\"{i}. {source}\")\n                    \n            except Exception as e:\n                st.error(f\"❌ Hata oluştu: {e}\")\n    else:\n        st.warning(\"⚠️ Lütfen bir soru girin.\")\n\n# Örnek sorular\nst.markdown(\"---\")\nst.markdown(\"### 💡 Örnek Sorular\")\ncol1, col2, col3 = st.columns(3)\n\nwith col1:\n    if st.button(\"Diyabet nedir?\"):\n        st.session_state.example_q = \"Diyabet nedir ve belirtileri nelerdir?\"\n\nwith col2:\n    if st.button(\"Migren nasıl geçer?\"):\n        st.session_state.example_q = \"Migren ağrısı nasıl geçer?\"\n\nwith col3:\n    if st.button(\"Hamilelik testleri\"):\n        st.session_state.example_q = \"Hamilelerde yapılan testler nelerdir?\"\n'''\n\n# Dosyayı kaydet\nwith open('app.py', 'w', encoding='utf-8') as f:\n    f.write(streamlit_code)\n\nprint(\"✅ app.py dosyası oluşturuldu!\")\nprint(\"📁 Dosya yolu: app.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:01:13.589662Z","iopub.execute_input":"2025-10-20T15:01:13.590040Z","iopub.status.idle":"2025-10-20T15:01:13.598957Z","shell.execute_reply.started":"2025-10-20T15:01:13.590019Z","shell.execute_reply":"2025-10-20T15:01:13.598098Z"}},"outputs":[{"name":"stdout","text":"✅ app.py dosyası oluşturuldu!\n📁 Dosya yolu: app.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 💾 Adım 11: FAISS Index ve Verileri Kaydet\n\nStreamlit uygulamasının kullanabilmesi için FAISS index, chunks ve metadataları kaydediyoruz.","metadata":{}},{"cell_type":"code","source":"# FAISS Index ve verileri kaydet\n\nimport pickle\nimport os\n\nprint(\"💾 FAISS index ve veriler kaydediliyor...\")\n\n# Tüm verileri bir dictionary'e koy\ndata_to_save = {\n    'index': index,\n    'chunks': chunks,\n    'metadatas': metadatas\n}\n\n# Pickle ile kaydet\nwith open('faiss_index.pkl', 'wb') as f:\n    pickle.dump(data_to_save, f)\n\nprint(\"✅ FAISS index kaydedildi!\")\nprint(\"📁 Dosya: faiss_index.pkl\")\nprint(f\"📊 Dosya boyutu: {round(os.path.getsize('faiss_index.pkl') / (1024*1024), 2)} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:01:13.599842Z","iopub.execute_input":"2025-10-20T15:01:13.600144Z","iopub.status.idle":"2025-10-20T15:01:15.726629Z","shell.execute_reply.started":"2025-10-20T15:01:13.600114Z","shell.execute_reply":"2025-10-20T15:01:15.725941Z"}},"outputs":[{"name":"stdout","text":"💾 FAISS index ve veriler kaydediliyor...\n✅ FAISS index kaydedildi!\n📁 Dosya: faiss_index.pkl\n📊 Dosya boyutu: 561.0 MB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## 📦 Adım 12: requirements.txt Oluştur\n\nDeploy için gerekli kütüphaneleri listeleyen dosyayı oluşturuyoruz.","metadata":{}},{"cell_type":"code","source":"# requirements.txt oluştur\n\nrequirements = \"\"\"streamlit==1.31.0\ngoogle-generativeai==0.3.2\nsentence-transformers==2.3.1\nfaiss-cpu==1.7.4\nnumpy==1.24.3\npandas==2.0.3\ndatasets==2.16.1\nhuggingface-hub==0.20.3\n\"\"\"\n\nwith open('requirements.txt', 'w') as f:\n    f.write(requirements)\n\nprint(\"✅ requirements.txt oluşturuldu!\")\nprint(\"\\n📋 İçerik:\")\nprint(requirements)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:01:15.728480Z","iopub.execute_input":"2025-10-20T15:01:15.728756Z","iopub.status.idle":"2025-10-20T15:01:15.733612Z","shell.execute_reply.started":"2025-10-20T15:01:15.728737Z","shell.execute_reply":"2025-10-20T15:01:15.732841Z"}},"outputs":[{"name":"stdout","text":"✅ requirements.txt oluşturuldu!\n\n📋 İçerik:\nstreamlit==1.31.0\ngoogle-generativeai==0.3.2\nsentence-transformers==2.3.1\nfaiss-cpu==1.7.4\nnumpy==1.24.3\npandas==2.0.3\ndatasets==2.16.1\nhuggingface-hub==0.20.3\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## 🎉 Proje Tamamlandı!\n\n### ✅ Yapılanlar:\n1. ✅ **Dataset yüklendi:** 42,804 Türkçe tıbbi makale\n2. ✅ **Veri temizlendi:** 41,788 temiz makale\n3. ✅ **Chunking yapıldı:** 244,150 parça oluşturuldu\n4. ✅ **Embedding model:** Multilingual MiniLM yüklendi\n5. ✅ **FAISS index:** Vector database oluşturuldu\n6. ✅ **RAG pipeline:** Soru-cevap sistemi çalışıyor\n7. ✅ **Streamlit app:** Web arayüzü hazır\n8. ✅ **Dosyalar:** app.py, faiss_index.pkl, requirements.txt\n\n### 📁 Oluşturulan Dosyalar:\n- `app.py` - Streamlit web uygulaması\n- `faiss_index.pkl` - FAISS vector database (561 MB)\n- `requirements.txt` - Gerekli kütüphaneler\n\n### 🚀 Sonraki Adımlar:\n1. **GitHub'a yükle:** Notebook + dosyalar\n2. **Deploy et:** Hugging Face Spaces veya Streamlit Cloud\n3. **README.md yaz:** Proje dokümantasyonu","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}